{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e5f12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T12:40:34.227680Z",
     "start_time": "2023-11-16T12:40:33.564147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997) - full transcript\n",
      "84 years later, a 100 year-old woman named Rose DeWitt Bukater tells the story to her granddaughter Lizzy Calvert, Brock Lovett, Lewis Bodine, Bobby Buell and Anatoly Mikailavich on the Keldysh about her life set in April 10th 1912, on a ship called Titanic when young Rose boards the departing ship with the upper-class passengers and her mother, Ruth DeWitt Bukater, and her fianc√©, Caledon Hockley. Meanwhile, a drifter and artist named Jack Dawson and his best friend Fabrizio De Rossi win third-class tickets to the ship in a game. And she explains the whole story from departure until the death of Titanic on its first and last voyage April 15th, 1912 at 2:20 in the morning.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Steps to scrape the webpage using Beautiful Soup\n",
    "# 1. use requests.get to get the response from the webpage\n",
    "# 2. save the text as content\n",
    "# 3. Use Beautiful Soup to download the html content using html.parser\n",
    "# 4. Find the element of the webpage using the BeautifulSoup parser using find()\n",
    "# 5. Save the data in the form of txt/csv using with open function.\n",
    "\n",
    "\n",
    "# Scrape a Website - https://subslikescript.com/movie/Titanic-120338\n",
    "# Lets use requests library to download the website html structure\n",
    "\n",
    "response = requests.get(\"https://subslikescript.com/movie/Titanic-120338\")\n",
    "#print(response) # We see that the response is 200 which means it is a success\n",
    "content = response.text # this is the page content\n",
    "headers = response.headers # get the headers\n",
    "#print(headers)\n",
    "\n",
    "# Now we want to download the html structure of the page..\n",
    "# For this we will use Beautiful Soup to scrap the website.\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "#print(soup.prettify()) # returns html content in a readable format\n",
    "\n",
    "# Finding an element ~ (name of the element, type of parser)\n",
    "box = soup.find('article', class_= \"main-article\")\n",
    "\n",
    "# We will go inside the box to find the title - On the webpage we see that h1 is under\n",
    "# the article. So now, we will use box to find the h1 tag and get the text from there\n",
    "title = box.find(\"h1\").get_text()\n",
    "print(title)\n",
    "\n",
    "# paragraph\n",
    "upper = box.find(\"p\", class_=\"plot\").get_text(separator = \" \", strip  =True)\n",
    "print(upper)\n",
    "\n",
    "# Similarly, we will go and extract the text from the webpage..the dialogues.\n",
    "pg = box.find(\"div\", class_= \"full-script\").get_text(separator = ' ', strip = True)\n",
    "#print(pg)\n",
    "\n",
    "# write the text in a txt file\n",
    "#with open(f'{title}.txt', \"w\") as file: # just write .csv instead of txt to get csv files\n",
    " #   file.write(pg)\n",
    "\n",
    "# write the text in a txt file with utf-8 encoding\n",
    "with open(f'{title}.txt', \"w\", encoding='utf-8') as file:\n",
    "    file.write(pg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737032d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
